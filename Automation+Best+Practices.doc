# Automation Best Practices

### This space is dedicated to providing concepts and guidelines to consider when creating/maintaining automated testing suites.

## Always have a Repeatable/Clean Start State
- Not tied to specific test data, if possible

## Leverage Parallelization:
- Run all tests in parallel if possible, (not running in serial) which should yield much faster results and be less prone to failures

## Leverage the "Page Object Model/Pattern" (POMs)
- Modularity / Maintainability

## Test Constantly:
- Run tests on every commit, yields a higher confidence in code changes
- Test as much as you can earlier in a sprint</span></li>
- Figure out how fast you want test suites to run, then determine the number of tests to run AND expand hardware : to expand throughput + value
- Retest all tests 2x (twice) to remove some false positives (of failures) - this often passes due to a smaller load by running only failures again

## Speed things up:
- Run tests under 5min (the average attention span of an engaged adult is less than 5min)
- Provide results before context is switched</span></li>
- Create an environment where DEVs work with you, not against yo=
- Engage audiences by providing validation results (value) sooner
- Keep commented out lines to a minimum
- Use consistent naming conventions
- Split up "long/multiline" methods if possible

## Start Small:
- Don't rush to automate every test case
- Identify test case priority, then automate
- Don't automate end-to-end (tests might be too long/complex/hard to maintain) - stub things out + combine (NOTE:: Except for UX tests!)

# Divide + Conquer (small, dynamic tests equals faster results)
Separate Testing Concerns:
- By functional area
- By hosted server
- By backend server

## Automation Development Activity
- Automation IS development effort
- Copy/Pasting inside code can be replaced by reusable code (Stay DRY: Don't Repeat Yourself)
- Source Control is cheap insurance (against lost code - commit often)
- Bad coding practices equates to bad testing practices (and vice versa)

## Don't forget to Wait:
- Use Explicit or Fluent waits (for single elements/moments of unknown duration)
- Use Implicit waits for waiting a set amount of the time (pre-known load duration/time)
- Fluent waits for variable element loading (like for lazy loaded pages)

## Avoid Dependencies:
- Tests should not rely on previous tests to run/pass/states (self-contained tests are better)
- One test's actions should not drive another test's assertion criteria (try not to use daisy-chaining, unless intended/necessary!)

## Prep Your Environment:
- Make sure your CI rig can handle the number of jobs running on it simultaneously
- Make sure your SUT *system under tests) can handle the number of tests running on it simultaneously/concurrently (ex: load tests)
- Try to emulate the REAL PROD environment (Simulate/Emulate PROD environment: Disk size, capacity, memory, speed, etc.)

## Collect Metrics:
- How long did test runs take before and after automation? (time span/duration of test)
- How many bugs do automated tests identify per release? (Would have escaped to PROD)
- How many engineers/time does a typical test run require (to run and resolve errors)?
- How much money/time does this automation save a company? Example: Calculate Return on Investment (ROI)

## Caching and InPrivate Browsing
- Sometimes, you might want to launch a browser in InPrivate mode if you want to make sure there's a clean cache for your tests (for, e.g., log in tests). 
- This is a super simple update to the BrowserWindow.Launch() method.
</br>
- Pass in "-private" as a second argument to the Launch method, so that it looks like:
- BrowserWindow.Launch(url, "-private")
- And voila, you have an InPrivate browser session with a clean cache!
